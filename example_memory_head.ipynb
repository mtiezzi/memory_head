{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7VItYI1KhCWNmGYC5wxy1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtiezzi/memory_head/blob/main/example_memory_head.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Introduction\n",
        "\n",
        "This notebook shows how to define and use Memory Heads (**MHs**).\n",
        "\n",
        "Let's start by cloning the repository containing the library."
      ],
      "metadata": {
        "id": "SDBDi_pDkOge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mtiezzi/memory_head.git"
      ],
      "metadata": {
        "id": "uMbz_KMIQpGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd memory_head/ocdi\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "VToiH_LpQuFo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries and utils"
      ],
      "metadata": {
        "id": "77N1jMBfRRye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from d2d.utils_2d_datasets import load_toy_dataset, fill_row_in_continual_confusion_matrix, \\\n",
        "    print_continual_confusion_matrix, update_metrics, print_metrics, check_validity_of_distributions, \\\n",
        "    create_metrics_dictionary, convert_toy_dataset_to_pytorch_tensors, create_continual_confusion_matrix, \\\n",
        "    plot_2d_data_and_predictions, add_keys_from_continual_memory_neurons"
      ],
      "metadata": {
        "id": "aRj7OVAlQ98A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's import a 2d dataset (MODES dataset, see the main paper for further details)."
      ],
      "metadata": {
        "id": "ZbCSh6XfRW-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------\n",
        "# CUSTOMIZABLE OPTIONS\n",
        "# --------------------\n",
        "device = torch.device('cpu')\n",
        "# file = \"d2d/generated_datasets/bi-modals_IID.npz\"\n",
        "file = \"d2d/generated_datasets/bi-modals_CI.npz\"\n",
        "# file = \"d2d/generated_datasets/bi-modals_CDI.npz\"\n",
        "# file = \"d2d/generated_datasets/bi-modals_CDID.npz\"\n",
        "# file = \"d2d/generated_datasets/bi-moons_IID.npz\"\n",
        "# file = \"d2d/generated_datasets/bi-moons_CI.npz\"\n",
        "# file = \"d2d/generated_datasets/bi-moons_CDI.npz\"\n",
        "# file = \"d2d/generated_datasets/bi-moons_CDID.npz\"\n",
        "# --------------------\n",
        "\n",
        "# loading data\n",
        "loaded_data, d, c = load_toy_dataset(file)  # loading data (returns: data dictionary, input size, num classes)\n",
        "\n",
        "# converting data collections (train, val, test) to torch tensors, right format, right device\n",
        "X_train, y_train, distributions_train, X_val, y_val, distributions_val, X_test, y_test, distributions_test = \\\n",
        "    convert_toy_dataset_to_pytorch_tensors(loaded_data, device)\n",
        "\n",
        "# checking validity of the distribution indices (better be sure!)\n",
        "check_validity_of_distributions(distributions_train, distributions_val, distributions_test)"
      ],
      "metadata": {
        "id": "AOdbwLiMRLZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create a vanilla MLP\n"
      ],
      "metadata": {
        "id": "TjCtdOIwRV96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the neural network\n",
        "net = nn.Sequential(\n",
        "    nn.Linear(in_features=d, out_features=10, bias=True),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(in_features=10, out_features=c, bias=True)\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "imx6I1a5UyIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test this model on the online domain-incremental 2d MODES dataset, and plot the decision surfaces."
      ],
      "metadata": {
        "id": "5VkD0JWZU5JM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.09)\n",
        "\n",
        "# continual confusion matrices (empty, -1 means \"unfilled yet\", important!)\n",
        "CCM_train = create_continual_confusion_matrix(distributions_train)\n",
        "CCM_val = create_continual_confusion_matrix(distributions_val)\n",
        "CCM_test = create_continual_confusion_matrix(distributions_test)\n",
        "\n",
        "# metrics containers (empty, -1 means \"unfilled yet\", important!)\n",
        "metrics_train = create_metrics_dictionary(distributions_train)\n",
        "metrics_val = create_metrics_dictionary(distributions_val)\n",
        "metrics_test = create_metrics_dictionary(distributions_test)\n",
        "\n",
        "# streaming loop\n",
        "net.train()\n",
        "for t, (x, y, distribution) in enumerate(zip(X_train, y_train, distributions_train)):\n",
        "\n",
        "    # adding batch size and clearing up the tensor box\n",
        "    x.unsqueeze_(0)\n",
        "    y.unsqueeze_(0)\n",
        "    distribution = distribution.item()\n",
        "\n",
        "    # learning step\n",
        "    o = net(x)\n",
        "    loss = nn.functional.cross_entropy(o, y, reduction='mean')  # no real reductions are actually performed...\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # printing\n",
        "    if t % 100 == 0:\n",
        "      print('t=' + str(t) + ', loss=' + str(loss.item()))\n",
        "\n",
        "    # metrics (computed when training ends or when the distribution is going to change on the next step)\n",
        "    if t == X_train.shape[0] - 1 or distributions_train[t + 1] != distribution:\n",
        "\n",
        "        # adding a new row to the continual confusion matrices (exploiting the current network)\n",
        "        fill_row_in_continual_confusion_matrix(CCM_train, distribution, net, X_train, y_train, distributions_train)\n",
        "        fill_row_in_continual_confusion_matrix(CCM_val, distribution, net, X_val, y_val, distributions_val)\n",
        "        fill_row_in_continual_confusion_matrix(CCM_test, distribution, net, X_test, y_test, distributions_test)\n",
        "\n",
        "        # updating metrics\n",
        "        update_metrics(metrics_train, CCM_train, distribution)\n",
        "        update_metrics(metrics_val, CCM_val, distribution)\n",
        "        update_metrics(metrics_test, CCM_test, distribution)\n",
        "\n",
        "        # printing continual confusion matrix\n",
        "        print_continual_confusion_matrix('train', CCM_train)\n",
        "        print_continual_confusion_matrix('val', CCM_val)\n",
        "        print_continual_confusion_matrix('test', CCM_test)\n",
        "\n",
        "        # printing metrics\n",
        "        print_metrics('train', metrics_train, distribution)\n",
        "        print_metrics('val', metrics_val, distribution)\n",
        "        print_metrics('test', metrics_test, distribution)\n",
        "\n",
        "# class dictionary\n",
        "class_dict = {'Class ' + str(j): j for j in range(0, c)}\n"
      ],
      "metadata": {
        "id": "jcyRSIPuUzu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# plotting predictions at the end of training\n",
        "plot_2d_data_and_predictions('train', X_train, y_train, class_dict, net).show()"
      ],
      "metadata": {
        "id": "BdCKm6rjWEf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is not able to discern the different data distributions: it catastrophically forgets previous knwoledge.\n",
        "\n",
        "\n",
        "Let's now test a Memory Head on the same task, defining it as follows:"
      ],
      "metadata": {
        "id": "KUMokyp8VVdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mh.layers import Linear as MHLinear\n",
        "from d2d.utils_2d_datasets import add_keys_from_mh\n",
        "net = nn.Sequential(\n",
        "    # nn.Linear(in_features=d, out_features=10, bias=True),\n",
        "    # nn.Tanh(),\n",
        "    # nn.Linear(in_features=10, out_features=c, bias=True)\n",
        "    MHLinear(in_features=d, out_features=c, shared_keys=True, bias=True,\n",
        "             key_mem_units=10,\n",
        "             psi_fn=\"identity\",\n",
        "             # choices=[\"identity\", \"identity_psi\", \"resize2d_sign\", \"resize1d\", \"resize2d\", \"sign\",  \"2d_resize_psi\"])\n",
        "             upd_m=\"WTA\",  # choices=[\"vanilla\", \"WTA\"]\n",
        "             upd_k=\"ad_hoc_WTA\",  # choices=[\"ad_hoc_WTA\", \"grad_WTA\", \"grad_not_WTA\"]\n",
        "             beta_k=0.01,\n",
        "             gamma_alpha=25.0, tau_alpha=0.95,\n",
        "             tau_mu=50,\n",
        "             tau_eta=50, scramble=True,\n",
        "             delta=2,\n",
        "             layer_norm=False,\n",
        "             distance=\"euclidean\",  # choices=[\"cosine\", \"euclidean\", \"dot_scaled\"]\n",
        "             )\n",
        "    ).to(device)\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "NKxKs4XbVol8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define metrics"
      ],
      "metadata": {
        "id": "y08Hmi1bV3Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# continual confusion matrices (empty, -1 means \"unfilled yet\", important!)\n",
        "CCM_train = create_continual_confusion_matrix(distributions_train)\n",
        "CCM_val = create_continual_confusion_matrix(distributions_val)\n",
        "CCM_test = create_continual_confusion_matrix(distributions_test)\n",
        "\n",
        "# metrics containers (empty, -1 means \"unfilled yet\", important!)\n",
        "metrics_train = create_metrics_dictionary(distributions_train)\n",
        "metrics_val = create_metrics_dictionary(distributions_val)\n",
        "metrics_test = create_metrics_dictionary(distributions_test)"
      ],
      "metadata": {
        "id": "HI6umIuFVvHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# streaming loop\n",
        "net.train()\n",
        "for t, (x, y, distribution) in enumerate(zip(X_train, y_train, distributions_train)):\n",
        "\n",
        "    # adding batch size and clearing up the tensor box\n",
        "    x.unsqueeze_(0)\n",
        "    y.unsqueeze_(0)\n",
        "    distribution = distribution.item()\n",
        "\n",
        "    # learning step\n",
        "    o = net(x)\n",
        "    loss = nn.functional.cross_entropy(o, y, reduction='mean')  # no real reductions are actually performed...\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # printing\n",
        "    if t % 100 == 0:\n",
        "      print('t=' + str(t) + ', loss=' + str(loss.item()))\n",
        "\n",
        "    # metrics (computed when training ends or when the distribution is going to change on the next step)\n",
        "    if t == X_train.shape[0] - 1 or distributions_train[t + 1] != distribution:\n",
        "        # adding a new row to the continual confusion matrices (exploiting the current network)\n",
        "        fill_row_in_continual_confusion_matrix(CCM_train, distribution, net, X_train, y_train, distributions_train)\n",
        "        fill_row_in_continual_confusion_matrix(CCM_val, distribution, net, X_val, y_val, distributions_val)\n",
        "        fill_row_in_continual_confusion_matrix(CCM_test, distribution, net, X_test, y_test, distributions_test)\n",
        "\n",
        "        # updating metrics\n",
        "        update_metrics(metrics_train, CCM_train, distribution)\n",
        "        update_metrics(metrics_val, CCM_val, distribution)\n",
        "        update_metrics(metrics_test, CCM_test, distribution)\n",
        "\n",
        "        # printing continual confusion matrix\n",
        "        print_continual_confusion_matrix('train', CCM_train)\n",
        "        print_continual_confusion_matrix('val', CCM_val)\n",
        "        print_continual_confusion_matrix('test', CCM_test)\n",
        "\n",
        "        # printing metrics\n",
        "        print_metrics('train', metrics_train, distribution)\n",
        "        print_metrics('val', metrics_val, distribution)\n",
        "        print_metrics('test', metrics_test, distribution)\n",
        "\n"
      ],
      "metadata": {
        "id": "jkQ_pB7JV1mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's plot the developed class separation surfaces, as long as the learned keys (blue stars)."
      ],
      "metadata": {
        "id": "n-8QYtfUWYaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class dictionary\n",
        "class_dict = {'Class ' + str(j): j for j in range(0, c)}\n",
        "\n",
        "# getting keys, if any, adding them to the data, with an ad-hoc label\n",
        "X_train, y_train, class_dict = add_keys_from_mh(net, X_train, y_train, class_dict)\n",
        "\n",
        "# plotting predictions at the end of training\n",
        "plot_2d_data_and_predictions('train', X_train, y_train, class_dict, net).show()"
      ],
      "metadata": {
        "id": "XSB_WP8UV87x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gf4eAGU1WKkB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}